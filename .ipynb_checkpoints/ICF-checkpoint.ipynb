{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import numpy\n",
    "import inspect\n",
    "import traceback\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradInputs\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder input (None, 784)\n",
      "fc fc1 784 128 sigmoid (None, 128)\n",
      "fc fc2 128 10 <function softmax at 0x000000000901EC18> (None, 10)\n",
      "lambda* test [None, 138]\n",
      "Softmax.0 Elemwise{add,no_inplace}.0\n",
      "Join.0\n"
     ]
    }
   ],
   "source": [
    "def nprand(shape, k):\n",
    "    return numpy.float32(numpy.random.uniform(-k,k, shape))\n",
    "\n",
    "def make_param(shape):\n",
    "    if len(shape) == 1:\n",
    "        return theano.shared(nprand(shape,0),'b')\n",
    "    elif len(shape) == 2:\n",
    "        return theano.shared(nprand(shape, numpy.sqrt(6./sum(shape))), 'W')\n",
    "    elif len(shape) == 4:\n",
    "        return theano.shared(nprand(shape, numpy.sqrt(6./(shape[1]+shape[0]*numpy.prod(shape[2:])))), 'W')\n",
    "    raise ValueError(shape)\n",
    "\n",
    "\n",
    "def _log(*args):\n",
    "    if _log.on:\n",
    "        print \" \".join(map(str,args))\n",
    "_log.on = True\n",
    "\n",
    "\n",
    "\n",
    "class BlockType:\n",
    "    def __init__(self, inputs=['input'], outputs=['output']):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "    def __call__(self, f):\n",
    "        self.f = f\n",
    "        sign = inspect.getargspec(f)\n",
    "        kwva = dict((a,default) for a,default in zip(sign.args[-len(sign.defaults):], sign.defaults))\n",
    "        tb = ''.join(traceback.format_stack())\n",
    "        \n",
    "        if not 'block' in kwva:\n",
    "            raise ValueError('block type',f,'does not have a block argument')\n",
    "        assert kwva['block'] is None, 'block is a reserved argument and should be None'\n",
    "        \n",
    "        def make_block(model,name='name',**kwargs):\n",
    "            block = Block(self, model, name)\n",
    "            kwargs['block'] = block\n",
    "            # replace named inputs with their corresponding blocks\n",
    "            for inp in self.inputs:\n",
    "                inp_name = kwargs[inp]\n",
    "                if inp_name in model.blocks:\n",
    "                    kwargs[inp] = model.blocks[inp_name]\n",
    "                    block.inputs.append(inp_name)\n",
    "                else:\n",
    "                    raise ValueError(\"Model's toposort is not sorted properly, '%s' requires '%s'='%s' but '%s' cannot be found or comes later\"%(name, inp, kwargs[inp], kwargs[inp]))\n",
    "            # call actual block maker\n",
    "            self.f(name,**kwargs)\n",
    "            return block\n",
    "        \n",
    "        def meta_make_block(name='name',**kwargs):\n",
    "            def f(model):\n",
    "                try:\n",
    "                    return make_block(model, name, **kwargs)\n",
    "                except Exception,e:\n",
    "                    print 'Block was created:'\n",
    "                    print tb\n",
    "                    traceback.print_exc()\n",
    "                    raise e\n",
    "            return f\n",
    "        meta_make_block.func_doc = f.func_doc\n",
    "        return meta_make_block\n",
    "\n",
    "class Block:\n",
    "    def __init__(self, blocktype, model, name):\n",
    "        self.blocktype = blocktype\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.paramList = []\n",
    "        self.inputs = []\n",
    "    def param(self, shape):\n",
    "        p = make_param(shape)\n",
    "        p.block = self\n",
    "        self.paramList.append(p)\n",
    "        self.model.registerParam(p)\n",
    "        return p\n",
    "    def __repr__(self):\n",
    "        return '<Block %s>'%self.name\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.blocks = {}\n",
    "        self.params = []\n",
    "    def registerParam(self, p):\n",
    "        self.params.append(p)\n",
    "    def build(self, description):\n",
    "        # we're going to assume that `description` is already\n",
    "        # correctly correctly sorted\n",
    "        self.toposort = []\n",
    "        for maker in description:\n",
    "            block = maker(self)\n",
    "            if block.name in self.blocks:\n",
    "                raise ValueError(\"Trying to add block '%s' to Model instance but block name already exists\"%block.name)\n",
    "            self.blocks[block.name] = block\n",
    "            self.toposort.append(block)\n",
    "\n",
    "    def apply(self, inputs, partial=False):\n",
    "        activation_cache = inputs\n",
    "        for block in self.toposort:\n",
    "            if block.name in inputs:\n",
    "                continue\n",
    "\n",
    "            # check if all inputs are available\n",
    "            skip = False\n",
    "            for i in block.inputs:\n",
    "                if i not in activation_cache:\n",
    "                    if partial:\n",
    "                        _log('skipping',block.name,'due to partial evaluation')\n",
    "                        skip = True\n",
    "                    else: raise ValueError(\"I tried using block '%s', needed by '%s', but it is missing (maybe you want partial=True?)\"%(i,block.name))\n",
    "            if skip: continue\n",
    "\n",
    "            # construct arg list and retrive the block's output\n",
    "            block_inputs = [activation_cache[i] for i in block.inputs]\n",
    "            outputs = block.output(*block_inputs)\n",
    "\n",
    "            if outputs is None: continue # the block does not want to be registered\n",
    "            \n",
    "            if not isinstance(outputs, list) and not isinstance(outputs, tuple): outputs = [outputs]\n",
    "            # fill the activation cache with the block's outputs\n",
    "            activation_cache[block.name] = outputs[0]\n",
    "            for output, output_name in zip(outputs, block.blocktype.outputs):\n",
    "                activation_cache[block.name+'.'+output_name] = output\n",
    "        return activation_cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "@BlockType(inputs=[])\n",
    "def placeholder(name='placeholder', shape=(None, 32, 32, 3), block=None):\n",
    "    _log('placeholder', name, shape)\n",
    "    block.output = lambda: None\n",
    "    block.output_shape = shape\n",
    "    \n",
    "@BlockType(outputs=['output','preact'])\n",
    "def fc(name='fclayer', input='input', nout=128, act=T.tanh, block=None):\n",
    "    \"\"\"Build a fully connected layer\n",
    "    name -- block name\n",
    "    input -- input block's name\n",
    "    nout -- number of outgoing units\n",
    "    act -- the activation function\n",
    "    \"\"\"\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nin, nout))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    prop = lambda x: (act(T.dot(x,W)+b), T.dot(x,W)+b)\n",
    "    block.output = prop\n",
    "    block.output_shape = input.output_shape[0], nout\n",
    "    _log('fc',name,nin, nout, act, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType()\n",
    "def conv(name='conv',input='input', nout=32, fs=5, act=T.nnet.relu, stride=(1,1),block=None):\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nout, nin, fs, fs))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    prop = lambda x: act(T.nnet.conv2d(x, W,\n",
    "                                       filter_shape=W.get_value().shape,\n",
    "                                       border_mode='half',\n",
    "                                       subsample=stride)\n",
    "                         + b.dimshuffle('x',0,'x','x'))\n",
    "    block.output = prop\n",
    "    block.output_shape = (input.output_shape[0], nout,\n",
    "                          input.output_shape[2] / stride[0],\n",
    "                          input.output_shape[3] / stride[1])\n",
    "    _log('conv',name,nin,nout,fs,act,stride, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType()\n",
    "def conv_transpose(name='convT',input='input', nout=32, fs=5, act=T.tanh, stride=(1,1),block=None):\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nin, nout, fs, fs))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    convT = AbstractConv2d_gradInputs(border_mode='half',subsample=stride)\n",
    "    # not sure what those two last dimensions are supposed to be :(\n",
    "    prop = lambda x: act(convT(W, x, [x.shape[2]*stride[0], x.shape[3]*stride[1], 1, 1])\n",
    "                         + b.dimshuffle('x',0,'x','x'))\n",
    "    block.output = prop\n",
    "    block.output_shape = (input.output_shape[0], nout,\n",
    "                          input.output_shape[2] * stride[0],\n",
    "                          input.output_shape[3] * stride[1])\n",
    "    _log('convT',name,nin,nout,fs,act,stride, block.output_shape)\n",
    "                         \n",
    "@BlockType()\n",
    "def Lambda(name='lambda', input='input', func=lambda x:x, func_shape=lambda xshape:xshape, block=None):\n",
    "    block.output = func\n",
    "    block.output_shape = func_shape(input.output_shape)\n",
    "    _log('lambda', name, block.output_shape)\n",
    "\n",
    "def LambdaN(name='lambda*', inputs=['A','B'], func=lambda *x:x, func_shape=lambda *x:x):\n",
    "    fakekw = dict(('input%d'%i,j) for i,j in enumerate(inputs))\n",
    "    @BlockType(inputs=fakekw.keys())\n",
    "    def f(name='name',block=None, **kwargs):\n",
    "        block.output = func\n",
    "        block.output_shape = func_shape(*[kwargs[i].output_shape for i in fakekw.keys()])\n",
    "        _log('lambda*',name,block.output_shape)\n",
    "        \n",
    "    return f(name=name, func=func,func_shape=func_shape, **fakekw)\n",
    "    \n",
    "@BlockType(inputs=['A','B'])\n",
    "def concatenate(name='concat', A='A', B='B', axis=1, block=None):\n",
    "    block.output = lambda a,b:T.concatenate([a,b], axis=axis)\n",
    "    As = A.output_shape\n",
    "    Bs = B.output_shape\n",
    "    new_shape = [0] * len(As)\n",
    "    assert len(As) == len(Bs), 'A and B must have the same number of dimensions'\n",
    "    for i,(a,b) in enumerate(zip(As,Bs)):\n",
    "        if i != axis:\n",
    "            assert a==b, 'A and B must have the same shape along axis %d, A: %s, B: %s'%(i,As,Bs)\n",
    "            new_shape[i] = a\n",
    "        else:\n",
    "            new_shape[i] = a+b\n",
    "    block.output_shape = new_shape\n",
    "    _log('concat', name, axis, As, Bs, block.output_shape)\n",
    "    \n",
    "@BlockType()\n",
    "def image2flat(name='img2flat', input='input', block=None):\n",
    "    block.output = lambda x:x.reshape([x.shape[0], -1])\n",
    "    block.output_shape = input.output_shape[0], numpy.prod(input.output_shape[1:])\n",
    "    _log('image2flat', name, block.output_shape)\n",
    "\n",
    "@BlockType()\n",
    "def flat2image(name='flat2img', input='input', shape=(3,32,32), block=None):\n",
    "    block.output = lambda x:x.reshape([x.shape[0]]+list(shape))\n",
    "    block.output_shape = [input.output_shape[0]] + list(shape)\n",
    "    assert numpy.prod(shape) == input.output_shape[1],\"image shape does not match this block's input shape\"\n",
    "    _log('flat2image', name, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType(inputs=['input','htm1'])\n",
    "def rnn_core(name='rnn_layer', input='input', htm1='htm1', nhid=32, block=None):\n",
    "    nin = input.output_shape[-1]\n",
    "    Wx = block.param((nin, nhid))\n",
    "    Wh = block.param((nhid, nhid))\n",
    "    b = block.param((nhid,))\n",
    "    Wx.name += name; Wh.name += name; b.name += name;\n",
    "    prop = lambda x,htm1: T.tanh(T.dot(x,Wx)+T.dot(htm1,Wh) + b)\n",
    "    block.output = prop\n",
    "    block.output_shape = input.output_shape[0], nhid\n",
    "    _log('rnn core',name,nin, nhid, block.output_shape)\n",
    "\n",
    "\n",
    "    \n",
    "class adam:\n",
    "    def __init__(self,\n",
    "                 beta1 = 0.9, beta2 = 0.999, epsilon = 1e-4):\n",
    "        self.b1 = numpy.float32(beta1)\n",
    "        self.b2 = numpy.float32(beta2)\n",
    "        self.eps = numpy.float32(epsilon)\n",
    "\n",
    "    def __call__(self, params, grads, lr):\n",
    "        t = theano.shared(numpy.array(2., dtype = 'float32'))\n",
    "        updates = OrderedDict()\n",
    "        updates[t] = t + 1\n",
    "\n",
    "        for param, grad in zip(params, grads):\n",
    "            last_1_moment = theano.shared(numpy.float32(param.get_value() * 0))\n",
    "            last_2_moment = theano.shared(numpy.float32(param.get_value() * 0))\n",
    "\n",
    "            new_last_1_moment = T.cast((numpy.float32(1.) - self.b1) * grad + self.b1 * last_1_moment, 'float32')\n",
    "            new_last_2_moment = T.cast((numpy.float32(1.) - self.b2) * grad**2 + self.b2 * last_2_moment, 'float32')\n",
    "\n",
    "            updates[last_1_moment] = new_last_1_moment\n",
    "            updates[last_2_moment] = new_last_2_moment\n",
    "            updates[param] = (param - (lr * (new_last_1_moment / (numpy.float32(1.) - self.b1**t)) /\n",
    "                                      (T.sqrt(new_last_2_moment / (numpy.float32(1.) - self.b2**t)) + self.eps)))\n",
    "\n",
    "        return list(updates.items())\n",
    "\n",
    "    \n",
    "def example():\n",
    "\n",
    "    # First start with a Model object\n",
    "    model = Model()\n",
    "    # then build the blocks in a model\n",
    "    model.build([\n",
    "        # think of a placeholder as a T.tensor\n",
    "        placeholder('input', shape=(None, 28*28)),\n",
    "        # here we create two fully connected layers\n",
    "        # the first argument is always the name of the block, it must be unique\n",
    "        fc('fc1', input='input', nout=128, act=T.nnet.sigmoid),\n",
    "        # the other arguments depend on the block type\n",
    "        # generally, `input` is the name of this block's input block\n",
    "        fc('fc2', input='fc1', nout=10, act=T.nnet.softmax),\n",
    "\n",
    "        # it's also possible to define lambdas, the downside being you need to specify the shape\n",
    "        LambdaN('test', inputs=['fc1','fc2'],\n",
    "                func=lambda x,y:T.concatenate([x,y],axis=0),\n",
    "                func_shape=lambda x,y:[x[0],y[1]+x[1]])\n",
    "        # making custom blocks might be a cleaner alternative to lambdas\n",
    "    ])\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')\n",
    "\n",
    "    # apply the model to x, here {'input':x} means we're attributing x to be the output\n",
    "    # of the block 'input', but it could be any block, think of it as theano's givens\n",
    "    forward_pass = model.apply({'input':x})\n",
    "    # retreive the output of fc2 during the (symbolic) forward pass\n",
    "    pred = forward_pass['fc2']\n",
    "    # the previous line retrieves the default output, but a block can have more than one\n",
    "    # for example the `fc` block has two outputs, 'output' and 'preact' (before the activation)\n",
    "    preact = forward_pass['fc2.preact']\n",
    "    # pred is a theano Softmax.0, while preact is `T.dot(x,W) + b`, which is a theano Elemwise{add}.0\n",
    "    print pred, preact \n",
    "\n",
    "    print forward_pass['test']\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "\"\"\"\n",
    "os.environ['THEANO_FLAGS'] = \"floatX=float32\"   \n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu\"  \n",
    "os.environ['THEANO_FLAGS'] = \"force_device=True\"   \n",
    "\"\"\"\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,force_device=True,floatX=float32\"\n",
    "\n",
    "import time\n",
    "import numpy\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as pp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Squares:\n",
    "    def __init__(self, nsquares=1, size=5, side=2):\n",
    "        self.nsquares = nsquares\n",
    "        self.size = size # size of the observation\n",
    "        self.side = side # size of squares\n",
    "    @property\n",
    "    def nactions(self):\n",
    "        return 4\n",
    "    def genRandomSample(self):\n",
    "        \"\"\"\n",
    "        get a random (s,a,s') transition from the environment (assuming a uniform policy)\n",
    "        returns (state, action, next state)\n",
    "        \"\"\"\n",
    "        p_0 = pos = [randint(0,self.size-self.side,2) for i in range(self.nsquares)]\n",
    "        action = randint(0,self.nactions,self.nsquares)\n",
    "        delta = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        s_0 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_0[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        pos = [p+delta[action[i]] for i,p in enumerate(pos)]\n",
    "        pos = [numpy.minimum(numpy.maximum(p,0),self.size-self.side) for p in pos]\n",
    "        s_1 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_1[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        return (s_0.flatten(), action, s_1.flatten(), p_0, pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learn, env, niters):\n",
    "    mbsize = 64\n",
    "    losses = []\n",
    "    for i in range(niters):\n",
    "        s,a,sp,tf,tf1 = map(numpy.float32, zip(*[env.genRandomSample() for j in range(mbsize)]))\n",
    "        losses.append(learn(s,sp,numpy.int32(a)[:,0]))\n",
    "    return losses\n",
    "\n",
    "def extract_features(encoder, policy, env, niters):\n",
    "    mbsize = 1\n",
    "    latent_features = []\n",
    "    real_features = []\n",
    "    policies = []\n",
    "    for i in range(niters):\n",
    "        s,a,sp,tf,tf1 = map(numpy.float32, zip(*[env.genRandomSample() for j in range(mbsize)]))\n",
    "        real_features.append(tf[0].flatten() / 6. - 1)\n",
    "        latent_features.append(encoder(s)[0])\n",
    "        policies.append(policy(s)[0])\n",
    "    return latent_features, real_features, policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[2]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[array([0, 7])]\n",
      "[array([9, 5])]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADKCAYAAAC11LviAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACkRJREFUeJzt3VGIZYddx/Hvz93NrrutmGJbmk00qYRi8GFThlqNlNpYm1ZxKygkUIkijA+tplKQ1Zf4IvigtT6UwtrGBIwJkkYbJNjG2FIFCZ2ki0k6loSYJttdsykVG3xINubvw9zAdnd2J/eekzmz//v9wHLvPffMnD+Hs18OZ+6ZSVUhSbr4/cDUA0iSxmHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1sXs7N3ZJ9tY+DmznJiXpovcC//2dqnrzVutta9D3cYCfyvXbuUlJuuj9U93zrdeynpdcJKkJgy5JTQwKepIbknwzyZNJjow1lCRpfgsHPcku4NPAB4FrgJuSXDPWYJKk+Qw5Q38X8GRVPVVVLwF3A4fHGUuSNK8hQT8IPHvG6+OzZd8nyWqStSRrp3lxwOYkSRcyJOjZZNk5f/6oqo5W1UpVrexh74DNSZIuZEjQjwNXnPH6cuDEsHEkSYsaEvSvAVcnuSrJJcCNwH3jjCVJmtfCd4pW1ctJPgZ8EdgF3FZVj482mSRpLoNu/a+q+4H7R5pFkjSAd4pKUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNDPp96FouXzxxbFu394HLDm3r9nTx8tjc4Bm6JDVh0CWpCYMuSU0sHPQkVyT5cpL1JI8nuWXMwSRJ8xnyQ9GXgU9U1SNJ3gg8nOSBqvrGSLNJkuaw8Bl6VZ2sqkdmz18A1oGDYw0mSZrPKB9bTHIlcC3w0CbvrQKrAPvYP8bmJEmbGPxD0SRvAD4PfLyqvnf2+1V1tKpWqmplD3uHbk6SdB6Dgp5kDxsxv7Oq7h1nJEnSIoZ8yiXA54D1qvrkeCNJkhYx5Az9OuDXgfclOTb796GR5pIkzWnhH4pW1b8CGXEWSdIA3ikqSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1McofidZy+MBlh6YeQdqUx+YGz9AlqQmDLklNDA56kl1Jvp7kH8YYSJK0mDHO0G8B1kf4PpKkAQYFPcnlwC8Cnx1nHEnSooaeoX8K+H3glRFmkSQNsHDQk/wScKqqHt5ivdUka0nWTvPiopuTJG1hyBn6dcAvJ3kauBt4X5K/PnulqjpaVStVtbKHvQM2J0m6kIWDXlV/UFWXV9WVwI3AP1fVR0abTJI0Fz+HLklNjHLrf1V9BfjKGN9LkrQYz9AlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWpiUNCT/HCSe5L8R5L1JD891mCSpPkM/SPRfwH8Y1X9apJLgP0jzCRJWsDCQU/yQ8B7gN8AqKqXgJfGGUuSNK8hl1zeDjwP/FWSryf5bJIDI80lSZrTkKDvBt4JfKaqrgX+Fzhy9kpJVpOsJVk7zYsDNidJupAhQT8OHK+qh2av72Ej8N+nqo5W1UpVrexh74DNSZIuZOGgV9V/Ac8mecds0fXAN0aZSpI0t6Gfcvkd4M7ZJ1yeAn5z+EiSpEUMCnpVHQNWRppFkjSAd4pKUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktTEoKAn+b0kjyd5LMldSfaNNZgkaT4LBz3JQeB3gZWq+klgF3DjWINJkuYz9JLLbuAHk+wG9gMnho8kSVrEwkGvqm8Dfwo8A5wE/qeqvjTWYJKk+Qy55HIpcBi4CrgMOJDkI5ust5pkLcnaaV5cfFJJ0gUNueTy88B/VtXzVXUauBf4mbNXqqqjVbVSVSt72Dtgc5KkCxkS9GeAdyfZnyTA9cD6OGNJkuY15Br6Q8A9wCPAo7PvdXSkuSRJc9o95Iur6lbg1pFmkSQN4J2iktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1sWXQk9yW5FSSx85Y9qYkDyR5YvZ46es7piRpK6/lDP124Iazlh0BHqyqq4EHZ68lSRPaMuhV9VXgu2ctPgzcMXt+B/DhkeeSJM1p0Wvob62qkwCzx7eMN5IkaRG7X+8NJFkFVgH2sf/13pwkLa1Fz9CfS/I2gNnjqfOtWFVHq2qlqlb2sHfBzUmStrJo0O8Dbp49vxn4wjjjSJIW9Vo+tngX8G/AO5IcT/JbwJ8A70/yBPD+2WtJ0oS2vIZeVTed563rR55FkjSAd4pKUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJaiJVtX0bS54HvnWet38E+M62DXPxcL+cy32yOffL5jrslx+rqjdvtdK2Bv1CkqxV1crUc+w07pdzuU82537Z3DLtFy+5SFITBl2SmthJQT869QA7lPvlXO6TzblfNrc0+2XHXEOXJA2zk87QJUkDTB70JDck+WaSJ5McmXqenSLJ00keTXIsydrU80wlyW1JTiV57Ixlb0ryQJInZo+XTjnjFM6zX/4oybdnx8yxJB+acsbtluSKJF9Osp7k8SS3zJYvzfEyadCT7AI+DXwQuAa4Kck1U860w/xcVR1alo9cncftwA1nLTsCPFhVVwMPzl4vm9s5d78A/PnsmDlUVfdv80xTexn4RFX9BPBu4KOznizN8TL1Gfq7gCer6qmqegm4Gzg88UzaQarqq8B3z1p8GLhj9vwO4MPbOtQOcJ79stSq6mRVPTJ7/gKwDhxkiY6XqYN+EHj2jNfHZ8sEBXwpycNJVqceZod5a1WdhI3/xMBbJp5nJ/lYkn+fXZJpe2lhK0muBK4FHmKJjpepg55Nlvmxmw3XVdU72bgc9dEk75l6IO14nwF+HDgEnAT+bNpxppHkDcDngY9X1femnmc7TR3048AVZ7y+HDgx0Sw7SlWdmD2eAv6OjctT2vBckrcBzB5PTTzPjlBVz1XV/1XVK8BfsoTHTJI9bMT8zqq6d7Z4aY6XqYP+NeDqJFcluQS4Ebhv4pkml+RAkje++hz4BeCxC3/VUrkPuHn2/GbgCxPOsmO8Gq2ZX2HJjpkkAT4HrFfVJ894a2mOl8lvLJp9tOpTwC7gtqr640kH2gGSvJ2Ns3KA3cDfLOt+SXIX8F42fmPec8CtwN8Dfwv8KPAM8GtVtVQ/IDzPfnkvG5dbCnga+O1Xrx0vgyQ/C/wL8CjwymzxH7JxHX0pjpfJgy5JGsfUl1wkSSMx6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1IT/w93P1tHfRNlrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf2fe2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SquaresNew:\n",
    "    def __init__(self, nsquares=1, size=5, side=2):\n",
    "        self.nsquares = nsquares\n",
    "        self.size = size # size of the observation\n",
    "        self.side = side # size of squares\n",
    "    @property\n",
    "    def nactions(self):\n",
    "        return 4\n",
    "    def genRandomSample(self):\n",
    "        \"\"\"\n",
    "        get a random (s,a,s') transition from the environment (assuming a uniform policy)\n",
    "        returns (state, action, next state)\n",
    "        \"\"\"\n",
    "        p_0 = pos = [randint(0,self.size-self.side,2) for i in range(self.nsquares)]\n",
    "        action = randint(0,self.nactions,self.nsquares)\n",
    "        delta = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        s_0 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_0[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        pos = [p+delta[action[i]] for i,p in enumerate(pos)]\n",
    "        pos = [numpy.minimum(numpy.maximum(p,0),self.size-self.side) for p in pos]\n",
    "        s_1 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_1[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        return (s_0.flatten(), action, s_1.flatten(), p_0, pos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = SquaresNew(1,12,2) \n",
    "\n",
    "recons = []\n",
    "st = env.genRandomSample()\n",
    "\n",
    "st2 = env.genRandomSample()\n",
    "\n",
    "recons.append([st[0],st[0]])\n",
    "\n",
    "\n",
    "pp.imshow(numpy.hstack([recons[-1][0].reshape((env.size,env.size)), recons[-1][1].reshape((env.size,env.size))]), interpolation='none')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(env.genRandomSample()[0])\n",
    "print(env.genRandomSample()[1])\n",
    "print(env.genRandomSample()[2])\n",
    "print(env.genRandomSample()[3])\n",
    "print(env.genRandomSample()[4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder s_t (None, 144)\n",
      "flat2image s_t_image [None, 1, 12, 12]\n",
      "conv conv1 1 16 3 <function relu at 0x00000000090DFBA8> (1, 1) (None, 16, 12, 12)\n",
      "conv conv2 16 16 3 <function relu at 0x00000000090DFBA8> (1, 1) (None, 16, 12, 12)\n",
      "image2flat conv2_flat (None, 2304)\n",
      "fc h1 2304 32 <function relu at 0x00000000090DFBA8> (None, 32)\n",
      "fc h 32 6 Elemwise{tanh,no_inplace} (None, 6)\n",
      "convT convT2 16 16 3 <function relu at 0x00000000090DFBA8> (1, 1) (None, 16, 12, 12)\n",
      "convT convT1 16 1 3 <function <lambda> at 0x000000000FBE69E8> (1, 1) (None, 1, 12, 12)\n",
      "fc pi_act 32 24 <function <lambda> at 0x000000000FBE6B38> (None, 24)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-c9550e7babcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mlearn_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mact_selectivity_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mencode_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_st\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mreconstruct_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr_st\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\compile\\function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\compile\\pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1792\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1794\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m             defaults)\n\u001b[0;32m   1796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1472\u001b[0m                         optimizer, inputs, outputs)\n\u001b[0;32m   1473\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                     \u001b[0moptimizer_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mnb_nodes_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                 \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph, start_from)\u001b[0m\n\u001b[0;32m   2468\u001b[0m                         \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchange_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2469\u001b[0m                         \u001b[0mt_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2470\u001b[1;33m                         \u001b[0mlopt_change\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2471\u001b[0m                         \u001b[0mtime_opts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlopt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2472\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlopt_change\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[0;32m   1980\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mreplacements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\tensor\\opt.pyc\u001b[0m in \u001b[0;36mlocal_greedy_distributor\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m   6400\u001b[0m         \u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6401\u001b[0m         _change, candidate, num, denum = attempt_distribution(\n\u001b[1;32m-> 6402\u001b[1;33m             candidate, num, denum, out_type,)\n\u001b[0m\u001b[0;32m   6403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6404\u001b[0m         \u001b[0mchange\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0m_change\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\tensor\\opt.pyc\u001b[0m in \u001b[0;36mattempt_distribution\u001b[1;34m(factor, num, denum, out_type)\u001b[0m\n\u001b[0;32m   6338\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6339\u001b[0m         success, pos_pairs, neg_pairs = distribute_greedy(pos_pairs,\n\u001b[1;32m-> 6340\u001b[1;33m                                                           neg_pairs, [n], [], out_type)\n\u001b[0m\u001b[0;32m   6341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6342\u001b[0m             \u001b[0mchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\tensor\\opt.pyc\u001b[0m in \u001b[0;36mdistribute_greedy\u001b[1;34m(pos_pairs, neg_pairs, num, denum, out_type, minscore)\u001b[0m\n\u001b[0;32m   6310\u001b[0m     new_pos_pairs = list(itertools.starmap(local_mul_canonizer.simplify,\n\u001b[0;32m   6311\u001b[0m                                            [(n + num, d + denum, out_type) for (n, d)\n\u001b[1;32m-> 6312\u001b[1;33m                                             in pos_pairs]))\n\u001b[0m\u001b[0;32m   6313\u001b[0m     new_neg_pairs = list(itertools.starmap(local_mul_canonizer.simplify,\n\u001b[0;32m   6314\u001b[0m                                            [(n + num, d + denum, out_type) for (n, d)\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\tensor\\opt.pyc\u001b[0m in \u001b[0;36msimplify\u001b[1;34m(self, num, denum, out_type)\u001b[0m\n\u001b[0;32m   4785\u001b[0m             \u001b[1;31m#       simplification to help auditing when things go\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4786\u001b[0m             \u001b[1;31m#       wrong\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4787\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4788\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\tensor\\nnet\\nnet.pyc\u001b[0m in \u001b[0;36msoftmax_simplifier\u001b[1;34m(numerators, denominators)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda2\\lib\\site-packages\\theano\\gof\\utils.pyc\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                     return (type(self) == type(other) and\n\u001b[1;32m--> 195\u001b[1;33m                             \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m                             tuple(getattr(other, a) for a in props))\n\u001b[0;32m    197\u001b[0m                 \u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__eq__'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "env = Squares(1,12,2) \n",
    "N_latent = 6 # number of latent ICF\n",
    "convn = [16,16] # number of hidden conv channels\n",
    "convfs = 3 # filter size\n",
    "nhid = 32 # number of fc hidden\n",
    "rec_factor = 0.1 # factor of the reconstruction loss\n",
    "lr = theano.shared(numpy.array(0.0005,'float32'))\n",
    "    \n",
    "model = Model()\n",
    "model.build([\n",
    "    placeholder('s_t', shape=(None, env.size**2)),\n",
    "\n",
    "    # encoder\n",
    "    flat2image('s_t_image', input='s_t', shape=(1,env.size,env.size)),\n",
    "    conv('conv1', input='s_t_image', nout=convn[0], fs=convfs, act=T.nnet.relu, stride=(1,1)),\n",
    "    conv('conv2', input='conv1',     nout=convn[1], fs=convfs, act=T.nnet.relu, stride=(1,1)),\n",
    "    image2flat('conv2_flat', input='conv2'),\n",
    "    fc('h1', input='conv2_flat', nout=nhid, act=T.nnet.relu),\n",
    "    fc('h', input='h1', nout=N_latent, act=T.tanh),\n",
    "\n",
    "    # decoder\n",
    "    conv_transpose('convT2', input='conv2', nout=convn[0], fs=convfs,act=T.nnet.relu,stride=(1,1)),\n",
    "    conv_transpose('convT1', input='convT2', nout=1, fs=convfs,act=lambda x:x,stride=(1,1)),\n",
    "        \n",
    "    # actor policy\n",
    "    fc('pi_act', input='h1', nout=env.nactions * N_latent, act=lambda x:x),\n",
    "\n",
    "])\n",
    "\n",
    "### theano tensors \n",
    "st = T.matrix()\n",
    "stp1 = T.matrix()\n",
    "at = T.ivector()\n",
    "\n",
    "    \n",
    "### apply\n",
    "fp_st = model.apply({'s_t': st}, partial=True)\n",
    "fp_stp1 = model.apply({'s_t':stp1}, partial=True)\n",
    "# features and reconstruction at time t\n",
    "f_st = fp_st['h']\n",
    "r_st = fp_st['convT1']\n",
    "\n",
    "# at time t+1\n",
    "f_stp1 = fp_stp1['h']\n",
    "    \n",
    "# policies\n",
    "pi_act = T.nnet.softmax(fp_st['pi_act'].reshape((-1, env.nactions))).reshape((-1, N_latent, env.nactions))\n",
    "\n",
    "# probabilities of the taken actions\n",
    "prob_act = pi_act[T.arange(st.shape[0]), :, at]\n",
    "\n",
    "\n",
    "### losses\n",
    "reconstruction_loss = T.mean((st.flatten()-r_st.flatten())**2)\n",
    "    \n",
    "    \n",
    "def sample_selectivity(f, fp):\n",
    "    return (f - fp) / (1e-4 + T.sum(T.nnet.relu(f - fp), axis=1)[:, None])\n",
    "\n",
    "    \n",
    "sel = sample_selectivity(f_st, f_stp1)\n",
    "selectivity_of_at = prob_act * sel[:,:N_latent]\n",
    "act_selectivity_loss = -T.mean(selectivity_of_at)\n",
    "total_loss = rec_factor * reconstruction_loss + act_selectivity_loss\n",
    "\n",
    "\n",
    "### theano functions\n",
    "params = model.params\n",
    "gradients = T.grad(total_loss, params)\n",
    "updates = adam()(params, gradients, lr)\n",
    "\n",
    "learn_func = theano.function([st,stp1,at], [act_selectivity_loss, reconstruction_loss], updates=updates)\n",
    "encode_func = theano.function([st], f_st)\n",
    "reconstruct_func = theano.function([st],r_st)\n",
    "policy_func = theano.function([st], pi_act)\n",
    "\n",
    "\n",
    "### training\n",
    "all_losses = []\n",
    "features = []\n",
    "recons = []\n",
    "for epoch in range(20):\n",
    "    # train\n",
    "    losses = train(learn_func, env, 500)\n",
    "    # decay lr\n",
    "    lr.set_value(numpy.float32(lr.get_value() * 0.99))\n",
    "    print epoch, map(numpy.mean,zip(*losses))\n",
    "    all_losses += losses\n",
    "\n",
    "    # plotting\n",
    "    latent_features, real_features, policies = extract_features(encode_func, policy_func, env, 200)\n",
    "    features.append([latent_features, real_features])\n",
    "        \n",
    "    ntrue = len(real_features[0])\n",
    "    nfeat = N_latent\n",
    "    feat = numpy.concatenate((latent_features,real_features),axis=1).T\n",
    "\n",
    "    real_features = numpy.float32(real_features).T\n",
    "    latent_features = numpy.float32(latent_features).T\n",
    "\n",
    "    # do a linear regression to get the coefficients and plot them\n",
    "    # to see how the real features correlate with the learned latent features\n",
    "    slopes = numpy.float32([\n",
    "        [scipy.stats.linregress(real, lat).slope\n",
    "        for real in real_features]\n",
    "        for lat in latent_features])\n",
    "    magnitudes = numpy.float32([abs(latent_features).mean(axis=1),\n",
    "                                latent_features.mean(axis=1),\n",
    "                                latent_features.var(axis=1)])\n",
    "    # see how well the reconstruction is doing\n",
    "    st = env.genRandomSample()[0]\n",
    "    rt = reconstruct_func([st])[0]\n",
    "    recons.append([st,rt])\n",
    "\n",
    "    policies_stats = numpy.mean(policies,axis=0)\n",
    "\n",
    "    # actual plotting\n",
    "    pp.clf()\n",
    "    f, axarr = pp.subplots(2,3,figsize=(19,8))\n",
    "    axarr[0,0].imshow(numpy.hstack([recons[-1][0].reshape((env.size,env.size)),\n",
    "                                    recons[-1][1].reshape((env.size,env.size))]), interpolation='none')\n",
    "    slopes_max = max([-slopes.min(), slopes.max()])\n",
    "    f.colorbar(axarr[1,1].imshow(slopes, interpolation='none', cmap='bwr',vmin=-slopes_max,vmax=slopes_max),\n",
    "               ax=axarr[1,1])\n",
    "    f.colorbar(axarr[1,0].imshow(policies_stats, interpolation='none',cmap='YlOrRd'), ax=axarr[1,0])\n",
    "    f.colorbar(axarr[0,1].imshow(magnitudes, interpolation='none',cmap='YlOrRd'), ax=axarr[0,1])\n",
    "        \n",
    "    for i in range(nfeat):\n",
    "        rf = np.arange(real_features.min(),real_features.max()+1/6.,1./6,'float32')\n",
    "        lf = numpy.float32([latent_features[i][np.int32(np.round(real_features[0]*12))==j].mean()\n",
    "                            for j in range(-12,8,2)])\n",
    "        axarr[0,2].plot(rf, lf)\n",
    "        indexes = sorted(range(latent_features.shape[1]), key=lambda x:real_features[1][x])\n",
    "        lf = numpy.float32([latent_features[i][np.int32(np.round(real_features[1]*12))==j].mean()\n",
    "                            for j in range(-12,8,2)])\n",
    "        axarr[1,2].plot(rf, lf)\n",
    "            \n",
    "    pp.savefig('plots/epoch_%03d.png'%epoch)\n",
    "    pp.show()\n",
    "    pp.close()\n",
    "return features, recons\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
