{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import numpy\n",
    "import inspect\n",
    "import traceback\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradInputs\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder input (None, 784)\n",
      "fc fc1 784 128 sigmoid (None, 128)\n",
      "fc fc2 128 10 <function softmax at 0x00000000091DB4A8> (None, 10)\n",
      "lambda* test [None, 138]\n",
      "Softmax.0 Elemwise{add,no_inplace}.0\n",
      "Join.0\n"
     ]
    }
   ],
   "source": [
    "def nprand(shape, k):\n",
    "    return numpy.float32(numpy.random.uniform(-k,k, shape))\n",
    "\n",
    "def make_param(shape):\n",
    "    if len(shape) == 1:\n",
    "        return theano.shared(nprand(shape,0),'b')\n",
    "    elif len(shape) == 2:\n",
    "        return theano.shared(nprand(shape, numpy.sqrt(6./sum(shape))), 'W')\n",
    "    elif len(shape) == 4:\n",
    "        return theano.shared(nprand(shape, numpy.sqrt(6./(shape[1]+shape[0]*numpy.prod(shape[2:])))), 'W')\n",
    "    raise ValueError(shape)\n",
    "\n",
    "\n",
    "def _log(*args):\n",
    "    if _log.on:\n",
    "        print \" \".join(map(str,args))\n",
    "_log.on = True\n",
    "\n",
    "\n",
    "\n",
    "class BlockType:\n",
    "    def __init__(self, inputs=['input'], outputs=['output']):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "    def __call__(self, f):\n",
    "        self.f = f\n",
    "        sign = inspect.getargspec(f)\n",
    "        kwva = dict((a,default) for a,default in zip(sign.args[-len(sign.defaults):], sign.defaults))\n",
    "        tb = ''.join(traceback.format_stack())\n",
    "        \n",
    "        if not 'block' in kwva:\n",
    "            raise ValueError('block type',f,'does not have a block argument')\n",
    "        assert kwva['block'] is None, 'block is a reserved argument and should be None'\n",
    "        \n",
    "        def make_block(model,name='name',**kwargs):\n",
    "            block = Block(self, model, name)\n",
    "            kwargs['block'] = block\n",
    "            # replace named inputs with their corresponding blocks\n",
    "            for inp in self.inputs:\n",
    "                inp_name = kwargs[inp]\n",
    "                if inp_name in model.blocks:\n",
    "                    kwargs[inp] = model.blocks[inp_name]\n",
    "                    block.inputs.append(inp_name)\n",
    "                else:\n",
    "                    raise ValueError(\"Model's toposort is not sorted properly, '%s' requires '%s'='%s' but '%s' cannot be found or comes later\"%(name, inp, kwargs[inp], kwargs[inp]))\n",
    "            # call actual block maker\n",
    "            self.f(name,**kwargs)\n",
    "            return block\n",
    "        \n",
    "        def meta_make_block(name='name',**kwargs):\n",
    "            def f(model):\n",
    "                try:\n",
    "                    return make_block(model, name, **kwargs)\n",
    "                except Exception,e:\n",
    "                    print 'Block was created:'\n",
    "                    print tb\n",
    "                    traceback.print_exc()\n",
    "                    raise e\n",
    "            return f\n",
    "        meta_make_block.func_doc = f.func_doc\n",
    "        return meta_make_block\n",
    "\n",
    "class Block:\n",
    "    def __init__(self, blocktype, model, name):\n",
    "        self.blocktype = blocktype\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.paramList = []\n",
    "        self.inputs = []\n",
    "    def param(self, shape):\n",
    "        p = make_param(shape)\n",
    "        p.block = self\n",
    "        self.paramList.append(p)\n",
    "        self.model.registerParam(p)\n",
    "        return p\n",
    "    def __repr__(self):\n",
    "        return '<Block %s>'%self.name\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.blocks = {}\n",
    "        self.params = []\n",
    "    def registerParam(self, p):\n",
    "        self.params.append(p)\n",
    "    def build(self, description):\n",
    "        # we're going to assume that `description` is already\n",
    "        # correctly correctly sorted\n",
    "        self.toposort = []\n",
    "        for maker in description:\n",
    "            block = maker(self)\n",
    "            if block.name in self.blocks:\n",
    "                raise ValueError(\"Trying to add block '%s' to Model instance but block name already exists\"%block.name)\n",
    "            self.blocks[block.name] = block\n",
    "            self.toposort.append(block)\n",
    "\n",
    "    def apply(self, inputs, partial=False):\n",
    "        activation_cache = inputs\n",
    "        for block in self.toposort:\n",
    "            if block.name in inputs:\n",
    "                continue\n",
    "\n",
    "            # check if all inputs are available\n",
    "            skip = False\n",
    "            for i in block.inputs:\n",
    "                if i not in activation_cache:\n",
    "                    if partial:\n",
    "                        _log('skipping',block.name,'due to partial evaluation')\n",
    "                        skip = True\n",
    "                    else: raise ValueError(\"I tried using block '%s', needed by '%s', but it is missing (maybe you want partial=True?)\"%(i,block.name))\n",
    "            if skip: continue\n",
    "\n",
    "            # construct arg list and retrive the block's output\n",
    "            block_inputs = [activation_cache[i] for i in block.inputs]\n",
    "            outputs = block.output(*block_inputs)\n",
    "\n",
    "            if outputs is None: continue # the block does not want to be registered\n",
    "            \n",
    "            if not isinstance(outputs, list) and not isinstance(outputs, tuple): outputs = [outputs]\n",
    "            # fill the activation cache with the block's outputs\n",
    "            activation_cache[block.name] = outputs[0]\n",
    "            for output, output_name in zip(outputs, block.blocktype.outputs):\n",
    "                activation_cache[block.name+'.'+output_name] = output\n",
    "        return activation_cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "@BlockType(inputs=[])\n",
    "def placeholder(name='placeholder', shape=(None, 32, 32, 3), block=None):\n",
    "    _log('placeholder', name, shape)\n",
    "    block.output = lambda: None\n",
    "    block.output_shape = shape\n",
    "    \n",
    "@BlockType(outputs=['output','preact'])\n",
    "def fc(name='fclayer', input='input', nout=128, act=T.tanh, block=None):\n",
    "    \"\"\"Build a fully connected layer\n",
    "    name -- block name\n",
    "    input -- input block's name\n",
    "    nout -- number of outgoing units\n",
    "    act -- the activation function\n",
    "    \"\"\"\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nin, nout))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    prop = lambda x: (act(T.dot(x,W)+b), T.dot(x,W)+b)\n",
    "    block.output = prop\n",
    "    block.output_shape = input.output_shape[0], nout\n",
    "    _log('fc',name,nin, nout, act, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType()\n",
    "def conv(name='conv',input='input', nout=32, fs=5, act=T.nnet.relu, stride=(1,1),block=None):\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nout, nin, fs, fs))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    prop = lambda x: act(T.nnet.conv2d(x, W,\n",
    "                                       filter_shape=W.get_value().shape,\n",
    "                                       border_mode='half',\n",
    "                                       subsample=stride)\n",
    "                         + b.dimshuffle('x',0,'x','x'))\n",
    "    block.output = prop\n",
    "    block.output_shape = (input.output_shape[0], nout,\n",
    "                          input.output_shape[2] / stride[0],\n",
    "                          input.output_shape[3] / stride[1])\n",
    "    _log('conv',name,nin,nout,fs,act,stride, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType()\n",
    "def conv_transpose(name='convT',input='input', nout=32, fs=5, act=T.tanh, stride=(1,1),block=None):\n",
    "    nin = input.output_shape[1]\n",
    "    W = block.param((nin, nout, fs, fs))\n",
    "    b = block.param((nout,))\n",
    "    W.name += name; b.name += name;\n",
    "    convT = AbstractConv2d_gradInputs(border_mode='half',subsample=stride)\n",
    "    # not sure what those two last dimensions are supposed to be :(\n",
    "    prop = lambda x: act(convT(W, x, [x.shape[2]*stride[0], x.shape[3]*stride[1], 1, 1])\n",
    "                         + b.dimshuffle('x',0,'x','x'))\n",
    "    block.output = prop\n",
    "    block.output_shape = (input.output_shape[0], nout,\n",
    "                          input.output_shape[2] * stride[0],\n",
    "                          input.output_shape[3] * stride[1])\n",
    "    _log('convT',name,nin,nout,fs,act,stride, block.output_shape)\n",
    "                         \n",
    "@BlockType()\n",
    "def Lambda(name='lambda', input='input', func=lambda x:x, func_shape=lambda xshape:xshape, block=None):\n",
    "    block.output = func\n",
    "    block.output_shape = func_shape(input.output_shape)\n",
    "    _log('lambda', name, block.output_shape)\n",
    "\n",
    "def LambdaN(name='lambda*', inputs=['A','B'], func=lambda *x:x, func_shape=lambda *x:x):\n",
    "    fakekw = dict(('input%d'%i,j) for i,j in enumerate(inputs))\n",
    "    @BlockType(inputs=fakekw.keys())\n",
    "    def f(name='name',block=None, **kwargs):\n",
    "        block.output = func\n",
    "        block.output_shape = func_shape(*[kwargs[i].output_shape for i in fakekw.keys()])\n",
    "        _log('lambda*',name,block.output_shape)\n",
    "        \n",
    "    return f(name=name, func=func,func_shape=func_shape, **fakekw)\n",
    "    \n",
    "@BlockType(inputs=['A','B'])\n",
    "def concatenate(name='concat', A='A', B='B', axis=1, block=None):\n",
    "    block.output = lambda a,b:T.concatenate([a,b], axis=axis)\n",
    "    As = A.output_shape\n",
    "    Bs = B.output_shape\n",
    "    new_shape = [0] * len(As)\n",
    "    assert len(As) == len(Bs), 'A and B must have the same number of dimensions'\n",
    "    for i,(a,b) in enumerate(zip(As,Bs)):\n",
    "        if i != axis:\n",
    "            assert a==b, 'A and B must have the same shape along axis %d, A: %s, B: %s'%(i,As,Bs)\n",
    "            new_shape[i] = a\n",
    "        else:\n",
    "            new_shape[i] = a+b\n",
    "    block.output_shape = new_shape\n",
    "    _log('concat', name, axis, As, Bs, block.output_shape)\n",
    "    \n",
    "@BlockType()\n",
    "def image2flat(name='img2flat', input='input', block=None):\n",
    "    block.output = lambda x:x.reshape([x.shape[0], -1])\n",
    "    block.output_shape = input.output_shape[0], numpy.prod(input.output_shape[1:])\n",
    "    _log('image2flat', name, block.output_shape)\n",
    "\n",
    "@BlockType()\n",
    "def flat2image(name='flat2img', input='input', shape=(3,32,32), block=None):\n",
    "    block.output = lambda x:x.reshape([x.shape[0]]+list(shape))\n",
    "    block.output_shape = [input.output_shape[0]] + list(shape)\n",
    "    assert numpy.prod(shape) == input.output_shape[1],\"image shape does not match this block's input shape\"\n",
    "    _log('flat2image', name, block.output_shape)\n",
    "\n",
    "\n",
    "@BlockType(inputs=['input','htm1'])\n",
    "def rnn_core(name='rnn_layer', input='input', htm1='htm1', nhid=32, block=None):\n",
    "    nin = input.output_shape[-1]\n",
    "    Wx = block.param((nin, nhid))\n",
    "    Wh = block.param((nhid, nhid))\n",
    "    b = block.param((nhid,))\n",
    "    Wx.name += name; Wh.name += name; b.name += name;\n",
    "    prop = lambda x,htm1: T.tanh(T.dot(x,Wx)+T.dot(htm1,Wh) + b)\n",
    "    block.output = prop\n",
    "    block.output_shape = input.output_shape[0], nhid\n",
    "    _log('rnn core',name,nin, nhid, block.output_shape)\n",
    "\n",
    "\n",
    "    \n",
    "class adam:\n",
    "    def __init__(self,\n",
    "                 beta1 = 0.9, beta2 = 0.999, epsilon = 1e-4):\n",
    "        self.b1 = numpy.float32(beta1)\n",
    "        self.b2 = numpy.float32(beta2)\n",
    "        self.eps = numpy.float32(epsilon)\n",
    "\n",
    "    def __call__(self, params, grads, lr):\n",
    "        t = theano.shared(numpy.array(2., dtype = 'float32'))\n",
    "        updates = OrderedDict()\n",
    "        updates[t] = t + 1\n",
    "\n",
    "        for param, grad in zip(params, grads):\n",
    "            last_1_moment = theano.shared(numpy.float32(param.get_value() * 0))\n",
    "            last_2_moment = theano.shared(numpy.float32(param.get_value() * 0))\n",
    "\n",
    "            new_last_1_moment = T.cast((numpy.float32(1.) - self.b1) * grad + self.b1 * last_1_moment, 'float32')\n",
    "            new_last_2_moment = T.cast((numpy.float32(1.) - self.b2) * grad**2 + self.b2 * last_2_moment, 'float32')\n",
    "\n",
    "            updates[last_1_moment] = new_last_1_moment\n",
    "            updates[last_2_moment] = new_last_2_moment\n",
    "            updates[param] = (param - (lr * (new_last_1_moment / (numpy.float32(1.) - self.b1**t)) /\n",
    "                                      (T.sqrt(new_last_2_moment / (numpy.float32(1.) - self.b2**t)) + self.eps)))\n",
    "\n",
    "        return list(updates.items())\n",
    "\n",
    "    \n",
    "def example():\n",
    "\n",
    "    # First start with a Model object\n",
    "    model = Model()\n",
    "    # then build the blocks in a model\n",
    "    model.build([\n",
    "        # think of a placeholder as a T.tensor\n",
    "        placeholder('input', shape=(None, 28*28)),\n",
    "        # here we create two fully connected layers\n",
    "        # the first argument is always the name of the block, it must be unique\n",
    "        fc('fc1', input='input', nout=128, act=T.nnet.sigmoid),\n",
    "        # the other arguments depend on the block type\n",
    "        # generally, `input` is the name of this block's input block\n",
    "        fc('fc2', input='fc1', nout=10, act=T.nnet.softmax),\n",
    "\n",
    "        # it's also possible to define lambdas, the downside being you need to specify the shape\n",
    "        LambdaN('test', inputs=['fc1','fc2'],\n",
    "                func=lambda x,y:T.concatenate([x,y],axis=0),\n",
    "                func_shape=lambda x,y:[x[0],y[1]+x[1]])\n",
    "        # making custom blocks might be a cleaner alternative to lambdas\n",
    "    ])\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')\n",
    "\n",
    "    # apply the model to x, here {'input':x} means we're attributing x to be the output\n",
    "    # of the block 'input', but it could be any block, think of it as theano's givens\n",
    "    forward_pass = model.apply({'input':x})\n",
    "    # retreive the output of fc2 during the (symbolic) forward pass\n",
    "    pred = forward_pass['fc2']\n",
    "    # the previous line retrieves the default output, but a block can have more than one\n",
    "    # for example the `fc` block has two outputs, 'output' and 'preact' (before the activation)\n",
    "    preact = forward_pass['fc2.preact']\n",
    "    # pred is a theano Softmax.0, while preact is `T.dot(x,W) + b`, which is a theano Elemwise{add}.0\n",
    "    print pred, preact \n",
    "\n",
    "    print forward_pass['test']\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "\"\"\"\n",
    "os.environ['THEANO_FLAGS'] = \"floatX=float32\"   \n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu\"  \n",
    "os.environ['THEANO_FLAGS'] = \"force_device=True\"   \n",
    "\"\"\"\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,force_device=True,floatX=float32\"\n",
    "\n",
    "import time\n",
    "import numpy\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as pp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Squares:\n",
    "    def __init__(self, nsquares=1, size=5, side=2):\n",
    "        self.nsquares = nsquares\n",
    "        self.size = size # size of the observation\n",
    "        self.side = side # size of squares\n",
    "    @property\n",
    "    def nactions(self):\n",
    "        return 4\n",
    "    def genRandomSample(self):\n",
    "        \"\"\"\n",
    "        get a random (s,a,s') transition from the environment (assuming a uniform policy)\n",
    "        returns (state, action, next state)\n",
    "        \"\"\"\n",
    "        p_0 = pos = [randint(0,self.size-self.side,2) for i in range(self.nsquares)]\n",
    "        action = randint(0,self.nactions,self.nsquares)\n",
    "        delta = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        s_0 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_0[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        pos = [p+delta[action[i]] for i,p in enumerate(pos)]\n",
    "        pos = [numpy.minimum(numpy.maximum(p,0),self.size-self.side) for p in pos]\n",
    "        s_1 = numpy.zeros([self.size]*2, 'float32')\n",
    "        for i in range(self.nsquares):\n",
    "            s_1[pos[i][0]:pos[i][0]+self.side,\n",
    "                pos[i][1]:pos[i][1]+self.side] = 1\n",
    "        return (s_0.flatten(), action, s_1.flatten(), p_0, pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learn, env, niters):\n",
    "    mbsize = 64\n",
    "    losses = []\n",
    "    for i in range(niters):\n",
    "        s,a,sp,tf,tf1 = map(numpy.float32, zip(*[env.genRandomSample() for j in range(mbsize)]))\n",
    "        losses.append(learn(s,sp,numpy.int32(a)[:,0]))\n",
    "    return losses\n",
    "\n",
    "def extract_features(encoder, policy, env, niters):\n",
    "    mbsize = 1\n",
    "    latent_features = []\n",
    "    real_features = []\n",
    "    policies = []\n",
    "    for i in range(niters):\n",
    "        s,a,sp,tf,tf1 = map(numpy.float32, zip(*[env.genRandomSample() for j in range(mbsize)]))\n",
    "        real_features.append(tf[0].flatten() / 6. - 1)\n",
    "        latent_features.append(encoder(s)[0])\n",
    "        policies.append(policy(s)[0])\n",
    "    return latent_features, real_features, policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder s_t (None, 144)\n",
      "flat2image s_t_image [None, 1, 12, 12]\n",
      "conv conv1 1 16 3 <function relu at 0x0000000009259438> (1, 1) (None, 16, 12, 12)\n",
      "conv conv2 16 16 3 <function relu at 0x0000000009259438> (1, 1) (None, 16, 12, 12)\n",
      "image2flat conv2_flat (None, 2304)\n",
      "fc h1 2304 32 <function relu at 0x0000000009259438> (None, 32)\n",
      "fc h 32 6 Elemwise{tanh,no_inplace} (None, 6)\n",
      "convT convT2 16 16 3 <function relu at 0x0000000009259438> (1, 1) (None, 16, 12, 12)\n",
      "convT convT1 16 1 3 <function <lambda> at 0x000000000B44ABA8> (1, 1) (None, 1, 12, 12)\n",
      "fc pi_act 32 24 <function <lambda> at 0x000000000B44ACF8> (None, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.005756564, 0.0062790732]\n",
      "1 [-0.076400474, 0.00087864557]\n",
      "2 [-0.15213421, 0.0003787961]\n",
      "3 [-0.15248512, 0.00021891735]\n",
      "4 [-0.15270784, 0.00016953048]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "env = Squares(1,12,2) \n",
    "N_latent = 6 # number of latent ICF\n",
    "convn = [16,16] # number of hidden conv channels\n",
    "convfs = 3 # filter size\n",
    "nhid = 32 # number of fc hidden\n",
    "rec_factor = 0.1 # factor of the reconstruction loss\n",
    "lr = theano.shared(numpy.array(0.0005,'float32'))\n",
    "    \n",
    "model = Model()\n",
    "model.build([\n",
    "    placeholder('s_t', shape=(None, env.size**2)),\n",
    "\n",
    "    # encoder\n",
    "    flat2image('s_t_image', input='s_t', shape=(1,env.size,env.size)),\n",
    "    conv('conv1', input='s_t_image', nout=convn[0], fs=convfs, act=T.nnet.relu, stride=(1,1)),\n",
    "    conv('conv2', input='conv1',     nout=convn[1], fs=convfs, act=T.nnet.relu, stride=(1,1)),\n",
    "    image2flat('conv2_flat', input='conv2'),\n",
    "    fc('h1', input='conv2_flat', nout=nhid, act=T.nnet.relu),\n",
    "    fc('h', input='h1', nout=N_latent, act=T.tanh),\n",
    "\n",
    "    # decoder\n",
    "    conv_transpose('convT2', input='conv2', nout=convn[0], fs=convfs,act=T.nnet.relu,stride=(1,1)),\n",
    "    conv_transpose('convT1', input='convT2', nout=1, fs=convfs,act=lambda x:x,stride=(1,1)),\n",
    "        \n",
    "    # actor policy\n",
    "    fc('pi_act', input='h1', nout=env.nactions * N_latent, act=lambda x:x),\n",
    "\n",
    "])\n",
    "\n",
    "### theano tensors \n",
    "st = T.matrix()\n",
    "stp1 = T.matrix()\n",
    "at = T.ivector()\n",
    "\n",
    "    \n",
    "### apply\n",
    "fp_st = model.apply({'s_t': st}, partial=True)\n",
    "fp_stp1 = model.apply({'s_t':stp1}, partial=True)\n",
    "# features and reconstruction at time t\n",
    "f_st = fp_st['h']\n",
    "r_st = fp_st['convT1']\n",
    "\n",
    "# at time t+1\n",
    "f_stp1 = fp_stp1['h']\n",
    "    \n",
    "# policies\n",
    "pi_act = T.nnet.softmax(fp_st['pi_act'].reshape((-1, env.nactions))).reshape((-1, N_latent, env.nactions))\n",
    "\n",
    "# probabilities of the taken actions\n",
    "prob_act = pi_act[T.arange(st.shape[0]), :, at]\n",
    "\n",
    "\n",
    "### losses\n",
    "reconstruction_loss = T.mean((st.flatten()-r_st.flatten())**2)\n",
    "    \n",
    "    \n",
    "def sample_selectivity(f, fp):\n",
    "    return (f - fp) / (1e-4 + T.sum(T.nnet.relu(f - fp), axis=1)[:, None])\n",
    "\n",
    "    \n",
    "sel = sample_selectivity(f_st, f_stp1)\n",
    "selectivity_of_at = prob_act * sel[:,:N_latent]\n",
    "act_selectivity_loss = -T.mean(selectivity_of_at)\n",
    "total_loss = rec_factor * reconstruction_loss + act_selectivity_loss\n",
    "\n",
    "\n",
    "### theano functions\n",
    "params = model.params\n",
    "gradients = T.grad(total_loss, params)\n",
    "updates = adam()(params, gradients, lr)\n",
    "\n",
    "learn_func = theano.function([st,stp1,at], [act_selectivity_loss, reconstruction_loss], updates=updates)\n",
    "encode_func = theano.function([st], f_st)\n",
    "reconstruct_func = theano.function([st],r_st)\n",
    "policy_func = theano.function([st], pi_act)\n",
    "\n",
    "\n",
    "### training\n",
    "all_losses = []\n",
    "features = []\n",
    "recons = []\n",
    "for epoch in range(20):\n",
    "    # train\n",
    "    losses = train(learn_func, env, 500)\n",
    "    # decay lr\n",
    "    lr.set_value(numpy.float32(lr.get_value() * 0.99))\n",
    "    print epoch, map(numpy.mean,zip(*losses))\n",
    "    all_losses += losses\n",
    "\n",
    "    # plotting\n",
    "    latent_features, real_features, policies = extract_features(encode_func, policy_func, env, 200)\n",
    "    features.append([latent_features, real_features])\n",
    "        \n",
    "    ntrue = len(real_features[0])\n",
    "    nfeat = N_latent\n",
    "    feat = numpy.concatenate((latent_features,real_features),axis=1).T\n",
    "\n",
    "    real_features = numpy.float32(real_features).T\n",
    "    latent_features = numpy.float32(latent_features).T\n",
    "\n",
    "    # do a linear regression to get the coefficients and plot them\n",
    "    # to see how the real features correlate with the learned latent features\n",
    "    slopes = numpy.float32([\n",
    "        [scipy.stats.linregress(real, lat).slope\n",
    "        for real in real_features]\n",
    "        for lat in latent_features])\n",
    "    magnitudes = numpy.float32([abs(latent_features).mean(axis=1),\n",
    "                                latent_features.mean(axis=1),\n",
    "                                latent_features.var(axis=1)])\n",
    "    # see how well the reconstruction is doing\n",
    "    st = env.genRandomSample()[0]\n",
    "    rt = reconstruct_func([st])[0]\n",
    "    recons.append([st,rt])\n",
    "\n",
    "    policies_stats = numpy.mean(policies,axis=0)\n",
    "\n",
    "    # actual plotting\n",
    "    pp.clf()\n",
    "    f, axarr = pp.subplots(2,3,figsize=(19,8))\n",
    "    axarr[0,0].imshow(numpy.hstack([recons[-1][0].reshape((env.size,env.size)),\n",
    "                                    recons[-1][1].reshape((env.size,env.size))]), interpolation='none')\n",
    "    slopes_max = max([-slopes.min(), slopes.max()])\n",
    "    f.colorbar(axarr[1,1].imshow(slopes, interpolation='none', cmap='bwr',vmin=-slopes_max,vmax=slopes_max),\n",
    "               ax=axarr[1,1])\n",
    "    f.colorbar(axarr[1,0].imshow(policies_stats, interpolation='none',cmap='YlOrRd'), ax=axarr[1,0])\n",
    "    f.colorbar(axarr[0,1].imshow(magnitudes, interpolation='none',cmap='YlOrRd'), ax=axarr[0,1])\n",
    "        \n",
    "    for i in range(nfeat):\n",
    "        rf = np.arange(real_features.min(),real_features.max()+1/6.,1./6,'float32')\n",
    "        lf = numpy.float32([latent_features[i][np.int32(np.round(real_features[0]*12))==j].mean()\n",
    "                            for j in range(-12,8,2)])\n",
    "        axarr[0,2].plot(rf, lf)\n",
    "        indexes = sorted(range(latent_features.shape[1]), key=lambda x:real_features[1][x])\n",
    "        lf = numpy.float32([latent_features[i][np.int32(np.round(real_features[1]*12))==j].mean()\n",
    "                            for j in range(-12,8,2)])\n",
    "        axarr[1,2].plot(rf, lf)\n",
    "            \n",
    "    pp.savefig('plots/epoch_%03d.png'%epoch)\n",
    "    pp.close()\n",
    "return features, recons\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
